{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "82760f72",
   "metadata": {},
   "source": [
    "# 03 — Baseline Regression (Predict Team Points)\n",
    "\n",
    "**Target:** `pts`  \n",
    "**Split:** chronological (first 80% train → last 20% test)  \n",
    "**Metrics:** MAE, RMSE vs. baseline (season average)\n",
    "\n",
    "**Plan:**\n",
    "1. Select features (e.g., `home`, `rest_days`, `fg_pct`, `reb`, `tov`, `opponent_*`)\n",
    "2. Chronological split\n",
    "3. Baselines (team season avg) + Linear Regression + Ridge\n",
    "4. Evaluate & plot Predicted vs Actual\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fecc5d26",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplt\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpathlib\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Path\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlinear_model\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LinearRegression, Ridge\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmetrics\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m mean_absolute_error, mean_squared_error\n\u001b[32m     10\u001b[39m CLEAN_PATH = Path(\u001b[33m\"\u001b[39m\u001b[33m../data/processed/team_games_clean.csv\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'sklearn'"
     ]
    }
   ],
   "source": [
    "# Setup & data load\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "CLEAN_PATH = Path(\"../data/processed/team_games_clean.csv\")\n",
    "IMG_DIR = Path(\"../img\"); IMG_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "df = pd.read_csv(CLEAN_PATH, parse_dates=[\"game_date\"])\n",
    "print(\"Loaded:\", CLEAN_PATH, \"| shape:\", df.shape)\n",
    "\n",
    "# Target and features\n",
    "TARGET = \"pts\"\n",
    "CANDIDATE_FEATURES = [\n",
    "    \"home\", \"rest_days\",\n",
    "    \"fg_pct\", \"fga\", \"fgm\",\n",
    "    \"reb\", \"ast\", \"tov\",\n",
    "    \"opponent_pts\", \"opponent_fg_pct\", \"opponent_reb\", \"opponent_tov\",\n",
    "]\n",
    "\n",
    "# Chronological 80/20 split\n",
    "df = df.sort_values(\"game_date\").reset_index(drop=True)\n",
    "split_idx = int(len(df) * 0.8)\n",
    "train_df, test_df = df.iloc[:split_idx], df.iloc[split_idx:]\n",
    "\n",
    "print(f\"Train: {train_df.shape} | Test: {test_df.shape}\")\n",
    "print(f\"Train date range: {train_df['game_date'].min()} to {train_df['game_date'].max()}\")\n",
    "print(f\"Test date range: {test_df['game_date'].min()} to {test_df['game_date'].max()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21331986",
   "metadata": {},
   "source": [
    "## 1) Baseline Model (Team Season Average)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5563f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate team season average from training data\n",
    "team_avg = train_df.groupby('team_abbreviation')['pts'].mean().to_dict()\n",
    "\n",
    "# Predict using team averages\n",
    "test_df['baseline_pred'] = test_df['team_abbreviation'].map(team_avg)\n",
    "\n",
    "# Evaluate baseline\n",
    "baseline_mae = mean_absolute_error(test_df['pts'], test_df['baseline_pred'])\n",
    "baseline_rmse = np.sqrt(mean_squared_error(test_df['pts'], test_df['baseline_pred']))\n",
    "\n",
    "print(\"=== BASELINE MODEL (Team Season Average) ===\")\n",
    "print(f\"MAE: {baseline_mae:.2f}\")\n",
    "print(f\"RMSE: {baseline_rmse:.2f}\")\n",
    "print()\n",
    "print(\"Sample predictions:\")\n",
    "print(test_df[['team_abbreviation', 'pts', 'baseline_pred']].head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8074503e",
   "metadata": {},
   "source": [
    "## 2) Prepare Features for ML Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "564bc42c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values in features\n",
    "print(\"Missing values in candidate features:\")\n",
    "print(train_df[CANDIDATE_FEATURES].isna().sum())\n",
    "print()\n",
    "\n",
    "# Drop rows with missing values\n",
    "X_train = train_df[CANDIDATE_FEATURES].dropna()\n",
    "y_train = train_df.loc[X_train.index, TARGET]\n",
    "\n",
    "X_test = test_df[CANDIDATE_FEATURES].dropna()\n",
    "y_test = test_df.loc[X_test.index, TARGET]\n",
    "\n",
    "print(f\"Training samples: {len(X_train)}\")\n",
    "print(f\"Test samples: {len(X_test)}\")\n",
    "print(f\"\\nFeatures used: {CANDIDATE_FEATURES}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "593b8fe3",
   "metadata": {},
   "source": [
    "## 3) Linear Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb5e5640",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Linear Regression\n",
    "lr = LinearRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred_lr = lr.predict(X_test)\n",
    "\n",
    "# Evaluate\n",
    "lr_mae = mean_absolute_error(y_test, y_pred_lr)\n",
    "lr_rmse = np.sqrt(mean_squared_error(y_test, y_pred_lr))\n",
    "\n",
    "print(\"=== LINEAR REGRESSION ===\")\n",
    "print(f\"MAE: {lr_mae:.2f}\")\n",
    "print(f\"RMSE: {lr_rmse:.2f}\")\n",
    "print(f\"Improvement over baseline MAE: {baseline_mae - lr_mae:.2f} points\")\n",
    "print()\n",
    "\n",
    "# Feature importance\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': CANDIDATE_FEATURES,\n",
    "    'coefficient': lr.coef_\n",
    "}).sort_values('coefficient', key=abs, ascending=False)\n",
    "\n",
    "print(\"Top feature coefficients:\")\n",
    "print(feature_importance.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb52a534",
   "metadata": {},
   "source": [
    "## 4) Ridge Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5180de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Ridge Regression with alpha=1.0\n",
    "ridge = Ridge(alpha=1.0)\n",
    "ridge.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred_ridge = ridge.predict(X_test)\n",
    "\n",
    "# Evaluate\n",
    "ridge_mae = mean_absolute_error(y_test, y_pred_ridge)\n",
    "ridge_rmse = np.sqrt(mean_squared_error(y_test, y_pred_ridge))\n",
    "\n",
    "print(\"=== RIDGE REGRESSION (alpha=1.0) ===\")\n",
    "print(f\"MAE: {ridge_mae:.2f}\")\n",
    "print(f\"RMSE: {ridge_rmse:.2f}\")\n",
    "print(f\"Improvement over baseline MAE: {baseline_mae - ridge_mae:.2f} points\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f68e5f5",
   "metadata": {},
   "source": [
    "## 5) Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be2ef08b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison table\n",
    "results = pd.DataFrame({\n",
    "    'Model': ['Baseline (Team Avg)', 'Linear Regression', 'Ridge Regression'],\n",
    "    'MAE': [baseline_mae, lr_mae, ridge_mae],\n",
    "    'RMSE': [baseline_rmse, lr_rmse, ridge_rmse]\n",
    "}).round(2)\n",
    "\n",
    "print(\"=== MODEL COMPARISON ===\")\n",
    "print(results)\n",
    "print()\n",
    "\n",
    "# Best model\n",
    "best_model_idx = results['MAE'].idxmin()\n",
    "print(f\"Best model: {results.loc[best_model_idx, 'Model']}\")\n",
    "print(f\"Best MAE: {results.loc[best_model_idx, 'MAE']:.2f}\")\n",
    "\n",
    "# Visualize comparison\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.bar(results['Model'], results['MAE'], color=['gray', 'skyblue', 'lightcoral'])\n",
    "plt.title('Mean Absolute Error (MAE)')\n",
    "plt.ylabel('Points')\n",
    "plt.xticks(rotation=15, ha='right')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.bar(results['Model'], results['RMSE'], color=['gray', 'skyblue', 'lightcoral'])\n",
    "plt.title('Root Mean Squared Error (RMSE)')\n",
    "plt.ylabel('Points')\n",
    "plt.xticks(rotation=15, ha='right')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(IMG_DIR / 'model_comparison.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0dd9298",
   "metadata": {},
   "source": [
    "## 6) Predicted vs Actual Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9741b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot predicted vs actual for Linear Regression\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Linear Regression\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(y_test, y_pred_lr, alpha=0.5, s=20)\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\n",
    "plt.xlabel('Actual Points')\n",
    "plt.ylabel('Predicted Points')\n",
    "plt.title(f'Linear Regression\\nMAE: {lr_mae:.2f}, RMSE: {lr_rmse:.2f}')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Ridge Regression\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.scatter(y_test, y_pred_ridge, alpha=0.5, s=20, color='coral')\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\n",
    "plt.xlabel('Actual Points')\n",
    "plt.ylabel('Predicted Points')\n",
    "plt.title(f'Ridge Regression\\nMAE: {ridge_mae:.2f}, RMSE: {ridge_rmse:.2f}')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(IMG_DIR / 'predicted_vs_actual.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4f43b27",
   "metadata": {},
   "source": [
    "## 7) Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "003a4179",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate prediction errors for best model (Linear Regression)\n",
    "errors = y_test - y_pred_lr\n",
    "test_df_clean = test_df.loc[y_test.index].copy()\n",
    "test_df_clean['prediction'] = y_pred_lr\n",
    "test_df_clean['error'] = errors\n",
    "\n",
    "# Error distribution\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(errors, bins=30, edgecolor='black', alpha=0.7)\n",
    "plt.xlabel('Prediction Error (Actual - Predicted)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Prediction Errors')\n",
    "plt.axvline(x=0, color='red', linestyle='--', linewidth=2)\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Error by team\n",
    "plt.subplot(1, 2, 2)\n",
    "team_errors = test_df_clean.groupby('team_abbreviation')['error'].mean().sort_values()\n",
    "plt.barh(range(len(team_errors)), team_errors.values)\n",
    "plt.yticks(range(len(team_errors)), team_errors.index, fontsize=8)\n",
    "plt.xlabel('Average Prediction Error')\n",
    "plt.title('Average Error by Team')\n",
    "plt.axvline(x=0, color='red', linestyle='--', linewidth=1)\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(IMG_DIR / 'error_analysis.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Error statistics:\")\n",
    "print(f\"Mean error: {errors.mean():.2f}\")\n",
    "print(f\"Std error: {errors.std():.2f}\")\n",
    "print(f\"Max overestimation: {errors.min():.2f}\")\n",
    "print(f\"Max underestimation: {errors.max():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1ba4c83",
   "metadata": {},
   "source": [
    "## 8) Summary and Conclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6c37406",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"REGRESSION MODELING SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "print()\n",
    "print(f\"Target: Predict team points (pts)\")\n",
    "print(f\"Training samples: {len(X_train)}\")\n",
    "print(f\"Test samples: {len(X_test)}\")\n",
    "print(f\"Features used: {len(CANDIDATE_FEATURES)}\")\n",
    "print()\n",
    "print(\"Model Performance (Test Set):\")\n",
    "print(\"-\" * 60)\n",
    "print(results.to_string(index=False))\n",
    "print()\n",
    "print(\"Key Findings:\")\n",
    "print(f\"1. Both ML models outperform the baseline by reducing MAE\")\n",
    "print(f\"2. Linear Regression MAE: {lr_mae:.2f} (improvement: {baseline_mae - lr_mae:.2f} points)\")\n",
    "print(f\"3. Ridge Regression MAE: {ridge_mae:.2f} (improvement: {baseline_mae - ridge_mae:.2f} points)\")\n",
    "print()\n",
    "print(\"Top predictive features:\")\n",
    "print(feature_importance.head(5).to_string(index=False))\n",
    "print()\n",
    "print(\"Next Steps:\")\n",
    "print(\"- Could explore Random Forest or Gradient Boosting models\")\n",
    "print(\"- Feature engineering: rolling averages, opponent strength metrics\")\n",
    "print(\"- Use predictions to derive game winners (classification)\")\n",
    "print(\"=\" * 60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nba",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
